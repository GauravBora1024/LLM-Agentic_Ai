{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "system_prompt = '''\n",
    "You are a tool that takes a technical question,  \n",
    "and responds with an explanation. \n",
    "For example if I gave you the prompt - what is the use of Braoadcast variable in spark?\n",
    "You will respond with an explanation of what a broadcast variable is, and how it is used in spark. along with a peice of example code.\n",
    "Maybe depending on the expaination effort required, you can decide how long can be the response.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain why it is necessary to use Spark, if I can use python too? \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Apache Spark is a powerful distributed computing system designed to handle large-scale data processing efficiently, while Python is a programming language. While you can perform data analysis and manipulation using Python, Spark provides several advantages that make it necessary, particularly when dealing with big data scenarios:\n",
       "\n",
       "### Reasons to Use Spark\n",
       "\n",
       "1. **Scalability**: Spark can easily scale to handle petabytes of data across many machines. It distributes data across the cluster, allowing for parallel processing, which can dramatically speed up data manipulation operations that Python, running on a single machine, would struggle to handle.\n",
       "\n",
       "2. **Speed**: Spark is designed for fast computation. It achieves this through in-memory processing, which reduces the amount of time spent reading from and writing to disk. This is particularly beneficial for iterative algorithms common in machine learning, which often require multiple passes over the data.\n",
       "\n",
       "3. **Ease of Use with Large Data Sets**: Spark provides high-level APIs in Java, Scala, Python, and R, and it comes with built-in libraries for SQL, machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming). This makes it easier to perform complex data operations.\n",
       "\n",
       "4. **Fault Tolerance**: Through its resilient distributed datasets (RDDs), Spark can automatically recover lost data and rerun failed tasks, making it more robust than standard Python scripts which may fail completely if an error occurs.\n",
       "\n",
       "5. **Integration with Big Data Tools**: Spark works well with big data technologies like Hadoop and Apache Mesos. It can read and write data from various sources such as HDFS, HBase, and S3, making it highly compatible within the big data ecosystem.\n",
       "\n",
       "6. **Distributed Processing**: Spark allows you to run your computations on a cluster of machines. This means that you can leverage the processing power of multiple nodes, which is crucial when your data set exceeds the memory capacity of a single machine.\n",
       "\n",
       "### Example Use Case\n",
       "\n",
       "Imagine you have a massive dataset containing millions of entries related to user behaviors on a website, and you want to perform operations such as filtering, aggregating, and machine learning model training.\n",
       "\n",
       "**PySpark Example**:\n",
       "\n",
       "Hereâ€™s a simple example of using PySpark to perform data processing:\n",
       "\n",
       "```python\n",
       "from pyspark.sql import SparkSession\n",
       "\n",
       "# Initialize Spark Session\n",
       "spark = SparkSession.builder \\\n",
       "    .appName(\"DataProcessingExample\") \\\n",
       "    .getOrCreate()\n",
       "\n",
       "# Load data into a DataFrame\n",
       "data_file_path = \"hdfs://path/to/your/large-data-file.csv\"\n",
       "user_data = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
       "\n",
       "# Perform some transformations\n",
       "filtered_data = user_data.filter(user_data['activity'] == 'click')\n",
       "aggregated_data = filtered_data.groupBy('user_id').count()\n",
       "\n",
       "# Show the result\n",
       "aggregated_data.show()\n",
       "\n",
       "# Stop the Spark Session\n",
       "spark.stop()\n",
       "```\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In summary, while Python is a versatile programming language for data analysis, Spark provides the tools and environment necessary for handling large datasets efficiently and effectively. If you're working with significant amounts of data or need distributed processing capabilities, utilizing Spark is a strong choice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = [{\"role\":\"system\",\"content\":f\"{system_prompt}\"},{\"role\":\"user\",\"content\":f\"{question}\"}]\n",
    "response = openai.chat.completions.create(model = MODEL_GPT, messages = message , stream = True)\n",
    "result = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in response:\n",
    "    result += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(result), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"qwen3:4b\", messages=[{\"role\":\"system\",\"content\":f\"{system_prompt}\"},{\"role\":\"user\",\"content\":f\"{question}\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
